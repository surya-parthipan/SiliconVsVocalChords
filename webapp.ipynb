{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = load_model('./model/audio_classifier_LSTM_model_v1.h5')\n",
    "\n",
    "def pad_audio_if_needed(audio, n_fft):\n",
    "    if len(audio) < n_fft:\n",
    "        padding = n_fft - len(audio)\n",
    "        audio = np.pad(audio, pad_width=(0, padding), mode='constant')\n",
    "    return audio\n",
    "\n",
    "def pad_or_truncate(array, max_length):\n",
    "        if array.shape[1] < max_length:\n",
    "            padding = max_length - array.shape[1]\n",
    "            array = np.pad(array, pad_width=((0, 0), (0, padding)), mode='constant')\n",
    "        else:\n",
    "            array = array[:, :max_length]\n",
    "        return array\n",
    "    \n",
    "def extract_features(file_path, max_time_steps=109, SAMPLE_RATE=16000, N_MELS=128):\n",
    "    audio, _ = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "    audio = pad_audio_if_needed(audio, n_fft=512)\n",
    "    \n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=SAMPLE_RATE, n_mels=N_MELS)\n",
    "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    \n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=SAMPLE_RATE, n_mfcc=13)\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=SAMPLE_RATE)\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=SAMPLE_RATE)\n",
    "    tonnetz = librosa.feature.tonnetz(y=audio, sr=SAMPLE_RATE)\n",
    "\n",
    "    # Pad or truncate each feature to have the same width\n",
    "    mel_spectrogram = pad_or_truncate(mel_spectrogram, max_time_steps)\n",
    "    mfccs = pad_or_truncate(mfccs, max_time_steps)\n",
    "    chroma = pad_or_truncate(chroma, max_time_steps)\n",
    "    spectral_contrast = pad_or_truncate(spectral_contrast, max_time_steps)\n",
    "    tonnetz = pad_or_truncate(tonnetz, max_time_steps)\n",
    "    \n",
    "    features = np.concatenate((mel_spectrogram, mfccs, chroma, spectral_contrast, tonnetz), axis=0)\n",
    "    return features\n",
    "\n",
    "def classify_audio(audio_file):\n",
    "    # Load audio file\n",
    "    extracted_features = np.array(extract_features(file_path=audio_file))\n",
    "    processed_audio = np.expand_dims(extracted_features, axis=0)\n",
    "    prediction = model.predict(processed_audio)\n",
    "    prediction = np.argmax(prediction, axis=1)\n",
    "    if prediction == 1:\n",
    "        return \"Human Voice\"\n",
    "    else:\n",
    "        return \"AI Generated Voice\"\n",
    "    \n",
    "# classify_audio('../audio-deepfake-detection-main/TestEvaluation/LA_E_4785445.flac')\n",
    "\n",
    "\n",
    "# Streamlit app\n",
    "st.title('Human vs AI Voice Classifier')\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Choose an audio file...\", type=['wav', 'mp3', 'ogg', 'flac'])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Call the classify_audio function\n",
    "    result = classify_audio(uploaded_file)\n",
    "    st.write(f'The audio file is classified as: {result}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
